{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x21c323a1cc8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import empty\n",
    "import numpy as np\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.float64\n",
      "tensor([[ 1.4316,  1.3041,  0.5361],\n",
      "        [-1.2588, -0.4268, -0.9840],\n",
      "        [-0.9309,  0.6688, -1.0878],\n",
      "        [-1.3049, -1.3529, -0.0602],\n",
      "        [-0.7150,  1.0859,  0.6840]], dtype=torch.float64)\n",
      "tensor([[ 1.4177],\n",
      "        [ 1.4022],\n",
      "        [ 0.4705],\n",
      "        [ 0.0183],\n",
      "        [-0.0564]], dtype=torch.float64)\n",
      "tensor([[-0.1569,  1.4790, -1.1713]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "M = torch.empty(5,3,dtype=float).normal_(0,1)\n",
    "print(M.size())\n",
    "print(M.dtype)\n",
    "print(M)\n",
    "x = torch.empty([5,1],dtype=float).normal_(0,1)\n",
    "print(x)\n",
    "print(x.t().mm(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [-2.1604e+16]])\n",
      "torch.FloatTensor\n",
      "tensor([0., 0., 0.])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "test = torch.empty(3,1)\n",
    "print(test)\n",
    "print(torch.zeros(3,1).type())\n",
    "print(torch.zeros(3))\n",
    "print(torch.zeros(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module() :\n",
    "    def __init__(self):\n",
    "        #???\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self,*gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    def param(self):\n",
    "        return []\n",
    "    def zero_grad(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "        One fully connected layer.\n",
    "        Input : input_dimension, output_dimension [int]\n",
    "        Those corresponds to the numbers of nodes in a given input layer\n",
    "        and the number of nodes in the output layer\n",
    "        Weights and Bias are initialized using the Xavier initialization\n",
    "        see course 5.5 slide 14 (What is the gain here??)\n",
    "    \"\"\"\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        std = math.sqrt(2./(in_dim+out_dim))\n",
    "        self.w = torch.empty(out_dim,in_dim).normal_(0,std)\n",
    "        self.b = torch.empty(out_dim,1).normal_(0,std)\n",
    "        self.dw = torch.zeros(out_dim,in_dim)\n",
    "        self.db = torch.zeros(out_dim,1)\n",
    "    \n",
    "    \"\"\"\n",
    "        Function to set grad to zero. Used in gradient step\n",
    "    \"\"\"\n",
    "    def zero_grad(self):\n",
    "        self.dw.zero_()\n",
    "        self.db.zero_()\n",
    "    \"\"\"\n",
    "        results of FW pass of layer, returns s = x*w + b\n",
    "        using formula from Practical 03\n",
    "    \"\"\"\n",
    "    def forward(self,x):\n",
    "        return self.w.mv(x.view(-1))+self.b\n",
    "    \n",
    "    \"\"\"\n",
    "        Backward pass, see practical 03\n",
    "    \"\"\"\n",
    "    def backward(self,x):\n",
    "        WRITE STUFF Here\n",
    "        \n",
    "#ACTIVATION FUNCTIONS\n",
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def backward(self,x,dl_dx):\n",
    "        ??\n",
    "\n",
    "class tanh(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x.tanh()\n",
    "    \n",
    "    def backward(self,x,dl_dx):\n",
    "        return dl_dx/ (torch.pow(torch.cosh(x),2))\n",
    "        \n",
    "        \n",
    "#LOSS         \n",
    "class MSEloss(Module):\n",
    "    \"\"\" SEE PRACTICAL 03 UPDATE DOC LATER\"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "     \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def forward(self,x,t):\n",
    "        return (x - t).pow(2).sum()\n",
    "    \n",
    "    def backward(self, x, t):\n",
    "        return 2 * (x - t)\n",
    "    # MAYBEWRONG UPDATE THIS LATER\n",
    "    \n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
