{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1d45650fbc8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import empty\n",
    "import numpy as np\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(0,10,1)\n",
    "for i, number in enumerate(a):\n",
    "    print(a[-i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [6, 4, 6],\n",
      "        [2, 2, 2]])\n",
      "tensor([[2, 3, 4],\n",
      "        [6, 4, 6],\n",
      "        [2, 2, 2]])\n",
      "tensor([[12, 12, 12],\n",
      "        [10, 10, 10],\n",
      "        [ 8,  8,  8]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[2,3,4],[3,2,3],[2,2,2]])\n",
    "b = torch.tensor([[1,1,1],[2,2,2],[1,1,1]])\n",
    "\n",
    "print(a*b)\n",
    "print(a.mul(b))\n",
    "print(a.matmul(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.float64\n",
      "tensor([[ 1.4316,  1.3041,  0.5361],\n",
      "        [-1.2588, -0.4268, -0.9840],\n",
      "        [-0.9309,  0.6688, -1.0878],\n",
      "        [-1.3049, -1.3529, -0.0602],\n",
      "        [-0.7150,  1.0859,  0.6840]], dtype=torch.float64)\n",
      "tensor([[ 1.4177],\n",
      "        [ 1.4022],\n",
      "        [ 0.4705],\n",
      "        [ 0.0183],\n",
      "        [-0.0564]], dtype=torch.float64)\n",
      "tensor([[-0.1569,  1.4790, -1.1713]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "M = torch.empty(5,3,dtype=float).normal_(0,1)\n",
    "print(M.size())\n",
    "print(M.dtype)\n",
    "print(M)\n",
    "x = torch.empty([5,1],dtype=float).normal_(0,1)\n",
    "print(x)\n",
    "print(x.t().mm(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [-2.1604e+16]])\n",
      "torch.FloatTensor\n",
      "tensor([0., 0., 0.])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "test = torch.empty(3,1)\n",
    "print(test)\n",
    "print(torch.zeros(3,1).type())\n",
    "print(torch.zeros(3))\n",
    "print(torch.zeros(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5529],\n",
      "        [-0.9859],\n",
      "        [ 0.8265],\n",
      "        [-0.0341],\n",
      "        [ 2.0729]])\n",
      "tensor([[ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True]])\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "s = torch.empty(5,1).normal_(0,1)\n",
    "print(s)\n",
    "print((s>0))\n",
    "print((s>0).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(5,2))\n",
    "print(torch.zeros(4,1))\n",
    "print(torch.zeros(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-5-6596a7c7dac7>, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-6596a7c7dac7>\"\u001b[1;36m, line \u001b[1;32m36\u001b[0m\n\u001b[1;33m    def param(self):\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Module() :\n",
    "    def __init__(self):\n",
    "        #???\n",
    "        pass\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self,*gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    def param(self):\n",
    "        return []\n",
    "    def zero_grad(self):\n",
    "        pass \n",
    "    #From stackoverflow : Must use pass to have an empty statement otherwise problem.\n",
    "    #Is it really useful to have it in Module? I think this is only used by Linear.\n",
    "    \n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "        One fully connected layer.\n",
    "        Input : input_dimension, output_dimension [int]\n",
    "        Those corresponds to the numbers of nodes in a given input layer\n",
    "        and the number of nodes in the output layer\n",
    "        Weights and Bias are initialized using the Xavier initialization\n",
    "        see course 5.5 slide 14 (What is the gain here??)\n",
    "    \"\"\"\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        Module.__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        std = math.sqrt(2./(in_dim+out_dim))\n",
    "        self.w = torch.empty(out_dim,in_dim).normal_(0,std)\n",
    "        self.b = torch.empty(out_dim).normal_(0,std)\n",
    "        self.dw = torch.zeros(out_dim,in_dim)\n",
    "        self.db = torch.zeros(out_dim)\n",
    "    \n",
    "     def param(self):\n",
    "        return torch.Tensor([self.w,self.dw,self.b,self.db])\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "            Function to set grad to zero. Used in gradient step\n",
    "        \"\"\"\n",
    "        self.dw.zero_()\n",
    "        self.db.zero_()\n",
    "        \n",
    "   \n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            results of FW pass of layer, returns s = x*w + b\n",
    "            using formula from Practical 03\n",
    "        \"\"\"\n",
    "        s = self.w.mv(x)+self.b\n",
    "        return s\n",
    "    \n",
    "    def backward(self,x,dl_ds):\n",
    "        \"\"\"\n",
    "            Backward pass, see practical 03 and slide 9,11 of course 3.6\n",
    "            Accumulates gradient according to chain rule, with mat product.\n",
    "            dl_ds is the derivative of loss wrt to s = x*w + b\n",
    "            dl_ds = dl_dx.mul(sigma'(s)) which is given to him the\n",
    "            backward of activation functions.\n",
    "        \"\"\"\n",
    "        self.dw.add_(dl_ds.view(-1, 1).mm(x.view(1, -1)))\n",
    "        self.db.add_(dl_ds.view(-1))\n",
    "        dl_dx_prev = self.w.t().mm(dl_ds)\n",
    "        return dl_dx_prev\n",
    "        \n",
    "#ACTIVATION FUNCTIONS\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "    \n",
    "    \n",
    "    def forward(self,s):\n",
    "        return torch.max(x,torch.zeros(x.size()))\n",
    "    \n",
    "    def backward(self,s,dl_dx):\n",
    "        \"\"\"\n",
    "            Def of backprop : dl_ds = dl_dx * dsigma(s)\n",
    "            dsigma for relu is f : (1, x>0, \n",
    "                                    0, x<0)\n",
    "            this should work as it gives a logical tensor then into float\n",
    "            not sure about dimensions.....\n",
    "        \"\"\"\n",
    "        return dl_dx * (s>0).float()\n",
    "        \n",
    "\n",
    "class Tanh(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x.tanh()\n",
    "    \n",
    "    def backward(self,s,dl_dx):\n",
    "        \"\"\" dtanh = 1/cosh^2\"\"\"\n",
    "        return dl_dx* (1/(torch.pow(torch.cosh(s),2)))\n",
    "        \n",
    "        \n",
    "#LOSS         \n",
    "class MSELoss(Module):\n",
    "    \"\"\" SEE PRACTICAL 03 UPDATE DOC LATER\"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        return (x - t).pow(2).sum()\n",
    "    \n",
    "    def backward(self, x, t):\n",
    "        #This is dl_dx. (dloss wrt to x for MSE loss is 2(x-t))\n",
    "        return 2 * (x - t)\n",
    "    # MAYBEWRONG UPDATE THIS LATER\n",
    "    \n",
    "class CrossEntropyLoss(Module):        \n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def softmax(x):\n",
    "        \"\"\"\n",
    "            Computes softmax with shift to be numerically stable for\n",
    "            large numbers or floats takes exp(x-max(x)) instead of exp(x)\n",
    "        \"\"\"\n",
    "            #this is really stablesoftmax(x)\n",
    "            #rather than softamx(x)\n",
    "        z = x- x.max()\n",
    "        exps = torch.exp(z)\n",
    "        return (exps/torch.sum(exps))\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        \"\"\"\n",
    "            With pj = exp(aj)/sum(exp(ak))\n",
    "            Loss = -Sum_j (yj) log(pj), \n",
    "            with t the target being the y in the formula\n",
    "            and pj = softmax(x)_j\n",
    "            log(p)*t does the element-wise product then we sum\n",
    "        \"\"\"\n",
    "        p = self.softmax(x)\n",
    "        sumResult = -torch.sum(torch.log(p)*t)\n",
    "        return sumResult\n",
    "    \n",
    "    def backward(self,x,t):\n",
    "        \"\"\"\n",
    "            computes dLoss \n",
    "            dl/dx_i = pi-yi from the slides\n",
    "        \"\"\"\n",
    "        p = self.softmax(x)\n",
    "        return p-t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
