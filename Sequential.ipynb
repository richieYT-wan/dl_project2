{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Sequential():\n",
    "    \"\"\"\n",
    "        An instance of the sequential class contains the Modules passed to it as arguments.\n",
    "        For example, pass linear, relu to it, it will be stored within its attribute \"members\"\n",
    "        the attribute memory is used to save the operations in the order in which they were done\n",
    "        to allow backprop (?)\n",
    "    \"\"\"\n",
    "    def __init__(self,*Modules):\n",
    "        self.members = []\n",
    "        self.params = []\n",
    "        for mod in Modules:\n",
    "            self.members.append(mod)\n",
    "            self.parameters.append(mod.param())\n",
    "        self.memory = []\n",
    "        self.lossmemory = []\n",
    "        \n",
    "    def param(self):\n",
    "        return self.parameters\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #MUST ADD THINGS TO MEMORY AFTER OPERATION.\n",
    "        # x is the input to which the operations of each module must be applied in the right order.\n",
    "        #Does this need to save the first entry ?\n",
    "        #It should save the value at each step (ex what is x1, s1, sigma(s1), etc.)\n",
    "        #this is done in the \n",
    "        #Should backprop also have another memory to save the backprop'd gradient and what else? Loss??\n",
    "        out = x\n",
    "        for module in self.members :\n",
    "            out = module.forward(out)\n",
    "            self.memory.append(out)\n",
    "        return out\n",
    "        \n",
    "    def backward(self, out, dl):\n",
    "        #must enumerate backwards in the list in memory. \n",
    "        #what should this do ?\n",
    "        #should save all the losses at each operation?\n",
    "        #backward (from Module) takes as argument : Z, dloss, where z is a\n",
    "        #can use for reversed(...) or use for i in () then use memory[-1-i] to iterate backward.\n",
    "        self.lossmemory.append(dl)\n",
    "        for idx, module in reversed(enumerate(self.members)):\n",
    "            #the input for backward is the value saved in memory \n",
    "            dl = module.backward(self.memory[i],dl)\n",
    "            self.lossmemory.append(dl)\n",
    "        #reversed loss memory\n",
    "        return self.lossmemory[::-1]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        #should only do zero grad for linear modules. \n",
    "        #If called on activation modules (like tanh), that does not have it specified,\n",
    "        #it should call the function definition from the mother class in which case it will pass.\n",
    "        for module in self.members :\n",
    "            module.zero_grad()\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
